#Baumgartner 2014 ReadME

1. Reference
Baumgartner, H. A. (2014) Understanding the Role of Non-Contrastive Variability in Word Learning and Visual Attention in Infancy [Doctoral dissertation, University of California, Davis]  ProQuest Dissertations Publishing, 3685177.

2. Abstract
Studies of word learning and comprehension in infancy often feature labeling events that feature a single exemplar of an object paired with a single label token, but the labeling events infants encounter in the real world are full of both acoustic and visual variability. Recent studies suggest that variability in phonemically non-contrastive features of speech (e.g., speaker voice, intonation) improves minimal pair learning at 14 months (Galle, Apfelbaum, & McMurray, 2014; Rost & McMurray, 2010). The present experiments investigate if the facilitative effect of acoustic variability in non-contrastive features extends to younger infants’ learning of dissimilar words, and if the effect is present for variability in non-contrastive visual features of objects. Twelve-month-old infants were habituated to novel object-label pairs featuring non-variable labels and objects (Experiment 1), variable labels paired with non-variable objects (Experiment 2), or non-variable labels paired with variable objects (Experiment 3). Following habituation, infants’ learning of the novel associations, as well as their ability to recognize familiar words, were then tested in a looking-while-listening task. Only infants who habituated to variable object exemplars in Experiment 3 demonstrated word learning at test. Recognition of familiar words was enhanced following habituation to variable stimuli in Experiments 2 and 3, consistent with other findings that infants’ ability to demonstrate linguistic knowledge is influenced by the broader context of the testing environment (e.g., Bergelson & Swingley, 2012; Fennell & Waxman, 2010; Fennell, 2012). Infants in Experiment 1 were also tested in a visual search task in which search items either did or did not vary in a task-irrelevant feature (Experiment 4). Infants who were better able to direct their attention to a shape-defined target in the more difficult, non-variable search condition showed more evidence of having learned the novel words in Experiment 1, providing evidence that visual attention and word learning are closely related.

3. Original study info
Infants were habituated to 2 novel object/label pairs (e.g,. FribbleA/"lif", FribbleB/"neem"). 
There were 3 habituation conditions: 
  1) Labels (single exemplar of novel object and single token of novel label)
  2) Colors (7 exemplars of novel object - varying in color - and single token of novel label)
  3) Speakers (single exemplar of novel object and 7 tokens of novel label - varying in speaker).
Infants then tested in looking while listening paradigm on novel words and familiar words (e.g., baby, doggy, ball). Audio on test trials was presented as "[Target_label]! [Frame][Target_label]? (e.g., Baby! Where's the baby?). NOTE: due to how the study was presented, visual stimuli during test trials were .mov files with target and distractor images on each side of screen, which makes it a little tricky to include paths to target images. I'm still figuring out the best way to handle this. Currently, all .mov and auditory stimuli are available in zipped file on OSF.

4. Importing decisions
Not sure what should go here (tried to make comments in code)

5. Importing ambiguity
QUESTION ABOUT TRIAL ORDER AND TRIAL TYPE ID (COMMENT IN CODE, LINE)